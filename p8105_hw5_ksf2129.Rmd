---
title: "p8105_hw5_ksf2129"
author: "Kimia Faroughi"
date: "2025-11-13"
output: github_document
---

Load packages

```{r, message = FALSE}
library(tidyverse)
library(patchwork)
```

## Problem 1

Function that randomly draws "birthdays" for each person in a fixed group size.

```{r}
bday_sim = function (n_room) {
  
  birthdays = sample(1:365, n_room, replace = TRUE)

  #if number of unique birthdays is < n_room (there is a repeated bday) this will return as true
  repeated_bday = length(unique(birthdays)) < n_room
  
  repeated_bday
  
}
```

Run function 10000 times for each group size between 2 and 50.

```{r}
bday_sim_results =
  expand_grid(
    bdays = 2:50, #number of people in the room
    iter = 1:10000 #number of simulation runs
  ) |> 
  mutate(
    result = map_lgl(bdays, bday_sim) #iterate
  ) |> 
  group_by(
    bdays
  ) |> 
  summarize(
    prob_repeat = mean(result) #probability at least two people in group share a birthday (avg across simulation runs)
  )
```

Plot showing probability as function of group size.

```{r, fig.height = 5, fig.width = 9}
bday_sim_results |> 
  ggplot(aes(x = bdays, y = prob_repeat)) +
  geom_point() +
  geom_line() +
  labs(
    x = "Group Size",
    y = "Probability of Shared Birthday"
  )
```

The plot shows that as group size increases, the probability that at least two people in the group share a birthday also increases. By the time the group consists of 50 people, this probability is close to 1, whereas when the group consists of 2 people the probability is close to 0. 

## Problem 2

Write function to simulate data

```{r}
sim_mean_sd = function(mu, n_subj = 30, sigma = 5) { #fix n and sigma
  
  sim_df = 
    tibble(
      x = rnorm(mean = mu, n = n_subj, sd = sigma) #normal distribution
    )
  
  sim_df |> 
    summarize(
      mu_hat = mean(x), 
      p_value = t.test(x) |> broom::tidy() |> pull(p.value) #default significance level 0.05, mu 0
    )
  
}
```

Set mu = 0 and generate 5000 datasets

```{r}
sim_results_0_df = 
  expand_grid(
    mu = 0,
    iter = 1:5000
  ) |> 
  mutate(
    results = map(mu, sim_mean_sd)
  ) |> 
  unnest(results)
```

Do this for mu = 1:6

```{r}
sim_results_df = 
  expand_grid(
    mu = 0:6,
    iter = 1:5000
  ) |> 
  mutate(
    results = map(mu, sim_mean_sd)
  ) |> 
  unnest(results)
```

Plot showing proportion of times null was rejected vs. true value of mu

```{r, fig.height = 5, fig.width = 9}
sim_results_df |> 
  group_by(mu) |>
  mutate(
    mu = str_c("mu = ", mu),
    reject = p_value < 0.05,
    prop_reject = mean(reject) #proportion that are true (p-value < 0.05) for each mu
  ) |>
  ggplot(aes(x = mu, y = prop_reject)) +
  geom_point() +
  labs(
    x = "Mu",
    y = "Proportion of Times Null Was Rejected (Power)"
  )
```

As effect size increases (distance of mu from 0), the power of the test (proportion of times the null hypothesis is rejected) also increases.

Plot of true value of mu vs average estimate of mu hat

```{r}
all_plot =
  sim_results_df |> 
    group_by(mu) |> 
    mutate(
      mu = str_c("mu = ", mu),
      estimated_mu_hat = mean(mu_hat)
    ) |> 
    ggplot(aes(x = mu, y = estimated_mu_hat)) +
    geom_point() +
    labs(
      x = "True Value of Mu",
      y = "Average Estimate of Mu Hat",
      title = "All Samples"
    ) +
    scale_y_continuous(breaks = c(0, 1, 2, 3, 4, 5, 6))
```

Plot of true value of mu vs average estimate of mu only in samples where null was rejected 

```{r}
sig_plot =
  sim_results_df |> 
    filter(p_value < 0.05) |>
    group_by(mu) |> 
    mutate(
      mu = str_c("mu = ", mu),
      estimated_mu_hat = mean(mu_hat)
    ) |> 
    ggplot(aes(x = mu, y = estimated_mu_hat)) +
    geom_point() +
    labs(
      x = "True Value of Mu",
      y = "Average Estimate of Mu Hat",
      title = "Samples Where Null Was Rejected"
    ) +
    scale_y_continuous(breaks = c(0, 1, 2, 3, 4, 5, 6))
```

Patchwork 

```{r, fig.height = 5, fig.width = 9}
mu_hat_plots = (all_plot + sig_plot)
mu_hat_plots
```

The sample average of mu hat across tests for which the null is rejected (p-value < 0.05) is not approximately equal to the true value of mu, especially for when the true value of mu is from 0-3. This is because tests where the null was rejected represent values of mu hat that are very unlikely to be consistent with an underlying distribution where the true value of mu is 0. As mu increases, the effect size increases which means the power of the test to result in the rejection of a false null hypothesis increases. This means that for true values of mu further from 0, the test has greater power to reject the null hypothesis that mu = 0, making the average estimate of mu hat closer to the actual value of mu. 

## Problem 3

Describe raw data

```{r, message = FALSE}
homicides_df = 
  read_csv("data/homicide-data.csv")

homicides_df
```

The raw data shows `r ncol(homicides_df)` variables describing a unique identifier, reported date of homicide, the victim's first and last name, race, age, sex, city, state, latitude and longitude, and disposition (the status of the case and perpetrator). These are described for `r nrow(homicides_df)` observations. 

Create variable for city + state

```{r}
#create city_state
homicides_df =
  homicides_df |> 
  mutate(city_state = str_c(city, state, sep = ", ")) |> 
  filter(city_state != "Tulsa, AL") #remove Tulsa, AL
```

Summarize total homicides and unsolved homicides within cities

```{r}
homicides_df |> 
  group_by(city_state) |> 
  summarize(
    total_homicides = n(),
    number_unsolved = sum(disposition == "Closed without arrest" | disposition == "Open/No arrest")
  ) |> 
  knitr::kable() #there is Tulsa, AL with just 1 obs, remove that above
```

Run `prop.test` for Baltimore, MD

```{r}
baltimore_df = 
  homicides_df |> 
  filter(city_state == "Baltimore, MD") |> 
  summarize(
    total_homicides = n(),
    number_unsolved = sum(disposition == "Closed without arrest" | disposition == "Open/No arrest")
  )

baltimore_test = 
  prop.test(baltimore_df |> pull(number_unsolved), baltimore_df |> pull(total_homicides))

baltimore_tidy = broom::tidy(baltimore_test)

unname(baltimore_tidy |> pull(estimate))
baltimore_tidy |> pull(conf.high)
baltimore_tidy |> pull(conf.low)
```

Run this for each city in the dataset

```{r}
#create new df with relevant vars
cities_df = 
  homicides_df |> 
  group_by(city_state) |> 
  summarize(
    total_homicides = n(),
    number_unsolved = sum(disposition == "Closed without arrest" | disposition == "Open/No arrest")
  )

#nest so new df of relevant info for each city
cities_nest =
  cities_df |> 
  nest(data = total_homicides:number_unsolved)
```

```{r}
#create function
cities_test = function(df) {
  
  test = 
    prop.test(df |> pull(number_unsolved), df |> pull(total_homicides))
  
  tidy_test = broom::tidy(test)
  
  results_df =
    tidy_test |> 
    mutate(
      prop_unsolved = unname(estimate),
      lower_ci = conf.high,
      upper_ci = conf.low
    ) |> 
    select(prop_unsolved, lower_ci, upper_ci)
  
  return(results_df)
  
}
```

```{r, warning = FALSE}
#map
unsolved_cities =
  cities_nest |> 
    mutate(
      cities_results = map(data, cities_test)
    ) |> 
    unnest()
```

Plot with estimates and CIs for each city

```{r, fig.height = 5, fig.width = 9}
unsolved_cities |> 
  mutate(
    city_state = fct_reorder(city_state, prop_unsolved)
  ) |> 
  ggplot(aes(x = city_state, y = prop_unsolved)) +
  geom_point() +
  geom_errorbar(aes(ymin = lower_ci, ymax = upper_ci)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 1)) +
  labs(
      x = "City",
      y = "Estimated Proportion of Unsolved Homicides\n and 95% CIs",
    ) 
```